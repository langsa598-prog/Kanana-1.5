{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Kanana Text Generation Demo (Colab)**\n",
        "\n",
        "ì¹´ì¹´ì˜¤ì˜ Kanana ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë© í™˜ê²½ì—ì„œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ì„œë¹„ìŠ¤ë¥¼ ì‹¤í–‰í•œë‹¤.\n",
        "\n",
        "## Pipeline\n",
        "\n",
        "ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•œ ë’¤, ì‚¬ìš©ì ì…ë ¥ì„ Chat Template í˜•íƒœë¡œ ë³€í™˜í•˜ê³  í† í°í™”í•œë‹¤.  \n",
        "ì´ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±(generate)í•˜ê³ , ìƒì„±ëœ í† í°ì„ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©í•œë‹¤.\n",
        "\n",
        "4bit ì–‘ìí™”ë¥¼ ì ìš©í•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì˜€ìœ¼ë©°, T4 GPU í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±í•˜ì˜€ë‹¤. ëª¨ë¸ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ë©° ì‘ë‹µì„ ìƒì„±í•œë‹¤.\n",
        "\n",
        "## Output\n",
        "\n",
        "ìµœì¢… ì¶œë ¥ê°’ì€ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ í…ìŠ¤íŠ¸ ì‘ë‹µì´ë©°, ì‚¬ìš©ìì˜ ì…ë ¥ì— ì´ì–´ì§€ëŠ” ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê°„ë‹¨í•œ ëŒ€í™”í˜• ì„œë¹„ìŠ¤ í˜•íƒœë¡œ ë™ì‘í•œë‹¤."
      ],
      "metadata": {
        "id": "bfjReA2CUQos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_K1HyKJUKUk"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "\n",
        "import os\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# ğŸ”¹ ëª¨ë¸ ì´ë¦„\n",
        "model_name = \"kakaocorp/kanana-1.5-8b-base\"\n",
        "\n",
        "# ğŸ”¹ 4bit ì–‘ìí™” (T4 í•„ìˆ˜)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        ")\n",
        "\n",
        "# ğŸ”¹ í† í¬ë‚˜ì´ì €\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ğŸ”¹ ëª¨ë¸ ë¡œë“œ\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        ")\n",
        "\n",
        "# ğŸ”¹ ëŒ€í™” ì…ë ¥\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"}\n",
        "]\n",
        "\n",
        "# ğŸ”¹ ì…ë ¥ ë³€í™˜\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "# ğŸ”¹ ëª¨ë¸ ìœ„ì¹˜ë¡œ ì´ë™\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# ğŸ”¹ ìƒì„± (ì†ë„ ì•ˆì • ì„¸íŒ…)\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=20,\n",
        "        do_sample=False,\n",
        "        use_cache=True\n",
        "    )\n",
        "\n",
        "# ğŸ”¹ ì¶œë ¥\n",
        "print(tokenizer.decode(\n",
        "    outputs[0][inputs[\"input_ids\"].shape[-1]:],\n",
        "    skip_special_tokens=True\n",
        "))"
      ]
    }
  ]
}