# Kanana Text Generation Demo (Colab)

ì¹´ì¹´ì˜¤ì˜ Kanana ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì½”ë© í™˜ê²½ì—ì„œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ì„œë¹„ìŠ¤ë¥¼ ì‹¤í–‰í•œë‹¤.

## Pipeline

ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•œ ë’¤, ì‚¬ìš©ì ì…ë ¥ì„ Chat Template í˜•íƒœë¡œ ë³€í™˜í•˜ê³  í† í°í™”í•œë‹¤.  
ì´ ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±(generate)í•˜ê³ , ìƒì„±ëœ í† í°ì„ ë‹¤ì‹œ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ë””ì½”ë”©í•œë‹¤.

4bit ì–‘ìí™”ë¥¼ ì ìš©í•˜ì—¬ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì˜€ìœ¼ë©°, T4 GPU í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±í•˜ì˜€ë‹¤. ëª¨ë¸ì€ ì‚¬ìš©ì ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ í† í°ì„ ìˆœì°¨ì ìœ¼ë¡œ ì˜ˆì¸¡í•˜ë©° ì‘ë‹µì„ ìƒì„±í•œë‹¤.

## Output

ìµœì¢… ì¶œë ¥ê°’ì€ ëª¨ë¸ì´ ìƒì„±í•œ ìì—°ì–´ í…ìŠ¤íŠ¸ ì‘ë‹µì´ë©°, ì‚¬ìš©ìì˜ ì…ë ¥ì— ì´ì–´ì§€ëŠ” ë¬¸ì¥ì„ ìƒì„±í•˜ëŠ” ê°„ë‹¨í•œ ëŒ€í™”í˜• ì„œë¹„ìŠ¤ í˜•íƒœë¡œ ë™ì‘í•œë‹¤.


!pip install -q transformers accelerate bitsandbytes

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ğŸ”¹ ëª¨ë¸ ì´ë¦„
model_name = "kakaocorp/kanana-1.5-8b-base"

# ğŸ”¹ 4bit ì–‘ìí™” (T4 í•„ìˆ˜)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
)

# ğŸ”¹ í† í¬ë‚˜ì´ì €
tokenizer = AutoTokenizer.from_pretrained(model_name)

# ğŸ”¹ ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    quantization_config=bnb_config,
)

# ğŸ”¹ ëŒ€í™” ì…ë ¥
messages = [
    {"role": "user", "content": "Who are you?"}
]

# ğŸ”¹ ì…ë ¥ ë³€í™˜
inputs = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt",
)

# ğŸ”¹ ëª¨ë¸ ìœ„ì¹˜ë¡œ ì´ë™
inputs = {k: v.to(model.device) for k, v in inputs.items()}

# ğŸ”¹ ìƒì„± (ì†ë„ ì•ˆì • ì„¸íŒ…)
with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=20,
        do_sample=False,
        use_cache=True
    )

# ğŸ”¹ ì¶œë ¥
print(tokenizer.decode(
    outputs[0][inputs["input_ids"].shape[-1]:],
    skip_special_tokens=True
))
